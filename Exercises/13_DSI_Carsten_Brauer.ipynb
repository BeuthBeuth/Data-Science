{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download the latest release and unzip it. Make a new directory under the user directory an copy all the unzipped folders and files into it.\n",
    "* Open the log file log4j.properties.template. Change INFO to WARN,  save and close the file. \n",
    "* In the same directory remove .template from every filename.\n",
    "* Go to Control Panel -> System and Security -> System -> Advanced Settings -> Environment Variables and create a new user variable named SPARK_HOME. Fill the value-field with the directory to Spark, e.g. C:\\Users\\XXXXX\\Spark.\n",
    "* Add %SPARK_HOME%\\bin to the path variable.\n",
    "* Download winutils, extract the content to c:\\winutils\\bin\n",
    "* Add HADOOP_HOME  -> C:\\winutils and %HADOOP_HOME%\\bin to the user/system variables\n",
    "* Make sure you have installed java\n",
    "* Open command line and run spark-shell. If that does not work: change into C:\\Users\\XXXX\\Spark\\bin and try again.\n",
    "\n",
    "![image.png](Spark.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Leider kriege ich es auf dem notebook nicht mehr zum Laufen, deswegen habe ich in VSC weitergemacht... im Mai lief es hier noch.\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from operator import add\n",
    "import re\n",
    "from English_Stop_Words import english_stop_words\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.host\", \"localhost\")\n",
    "conf.setMaster('local')\n",
    "conf.setAppName('Spark Basics Exercise')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "##### Einlesen des vorgesäuberten Textfiles ins RDD\n",
    "rdd_text = sc.textFile(\"t8.shakespeare_cleaned.txt\")\n",
    "\n",
    "##### Textbereinigung und Zählen der verschiedenen Wörter:\n",
    "wordcount = rdd_text \\\n",
    "    .flatMap(lambda line: line.split(' ')) \\\n",
    "    .map(lambda word: word.lower()) \\\n",
    "    .map(lambda word: re.sub(r'\\'+[a-z]', '', word)) \\\n",
    "    .map(lambda word: re.sub(r'[\\n\\b\\f\\r\\t\\v,.!?:;\\\"[\\]{}()&\\-\\'\\x00]', ' ', word)) \\\n",
    "    .map(lambda word: word.replace('\\\"', '')) \\\n",
    "    .map(lambda word: word.strip()) \\\n",
    "    .map(lambda word: re.sub(r'\\s+[a-z]+$', '', word)) \\\n",
    "    .map(lambda word: re.sub(r'[s]$', '', word)) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(add)\n",
    "\n",
    "print(\"Anzahl der Wörter: \" + str(wordcount.count()))\n",
    "\n",
    "##### Wir erstellen eine zweite Liste, bei der wir zum Vergleich die sog. stop-words und alle Wörter, die aus weniger als 3 Zeichen bestehen herausfiltern:\n",
    "filtered_wordcount = wordcount.filter(lambda x: len(x[0]) > 2 and not x[0] in english_stop_words)\n",
    "print(\"Anzahl der Wörter ohne Stop-Wörter: \" + str(filtered_wordcount.count()))\n",
    "\n",
    "##### Wir erstellen 2 Listen:\n",
    "most_frequent_words_filtered = filtered_wordcount.takeOrdered(24, key=lambda x: -x[1])\n",
    "wordcount = wordcount.filter(lambda x: len(x[0]) > 0)\n",
    "most_frequent_words = wordcount.takeOrdered(24, key=lambda x: -x[1])\n",
    "\n",
    "##### Wir erstellen 2 Textfiles, einmal die ungefilterte und einmal die gefilterte Liste\n",
    "with open('most_frequent_words.txt', 'w') as filehandle:\n",
    "    line_number = 1\n",
    "    for count_item in most_frequent_words:\n",
    "        filehandle.write(str(line_number) + \"\\t\" + '%s\\t\\t-\\tOccurrences: %d\\n' % (count_item[0], count_item[1]))\n",
    "        line_number = line_number +1\n",
    "\n",
    "with open('most_frequent_words_filtered.txt', 'w') as filehandle:\n",
    "    line_number = 1\n",
    "    for count_item in most_frequent_words_filtered:\n",
    "        filehandle.write(str(line_number) + \"\\t\" + '%s\\t\\t-\\tOccurrences: %d\\n' % (count_item[0], count_item[1]))\n",
    "        line_number = line_number +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output:\n",
    "Anzahl der Wörter: 20850\n",
    "Anzahl der Wörter ohne Stop-Wörter: 20356"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ungefiltert:\n",
    "\n",
    "*  1\ti\t\t-\tOccurrences: 31413\n",
    "*  2\tthe\t\t-\tOccurrences: 27319\n",
    "*  3\tand\t\t-\tOccurrences: 26023\n",
    "*  4\ta\t\t-\tOccurrences: 20203\n",
    "*  5\tto\t\t-\tOccurrences: 19736\n",
    "*  6\tof\t\t-\tOccurrences: 17471\n",
    "*  7\tyou\t\t-\tOccurrences: 13631\n",
    "*  8\tmy\t\t-\tOccurrences: 12464\n",
    "*  9\tthat\t-\tOccurrences: 11281\n",
    "* 10\tin\t\t-\tOccurrences: 11071\n",
    "* 11\tnot\t\t-\tOccurrences: 8499\n",
    "* 12\twith\t-\tOccurrences: 7779\n",
    "* 13\tme\t\t-\tOccurrences: 7778\n",
    "* 14\tit\t\t-\tOccurrences: 7722\n",
    "* 15\tfor\t\t-\tOccurrences: 7624\n",
    "* 16\tyour\t-\tOccurrences: 6901\n",
    "* 17\tbe\t\t-\tOccurrences: 6870\n",
    "* 18\thi\t\t-\tOccurrences: 6869\n",
    "* 19\the\t\t-\tOccurrences: 6600\n",
    "* 20\tthi\t\t-\tOccurrences: 6589\n",
    "* 21\tbut\t\t-\tOccurrences: 6256\n",
    "* 22\thave\t-\tOccurrences: 5888\n",
    "* 23\tthou\t-\tOccurrences: 5485\n",
    "* 24\thim\t\t-\tOccurrences: 5204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gefiltert\n",
    "\n",
    "*  1\tthi\t\t-\tOccurrences: 6589\n",
    "*  2\tthou\t-\tOccurrences: 5485\n",
    "*  3\tthy\t\t-\tOccurrences: 4031\n",
    "*  4\tshall\t-\tOccurrences: 3591\n",
    "*  5\tlord\t-\tOccurrences: 3555\n",
    "*  6\tthee\t-\tOccurrences: 3181\n",
    "*  7\tking\t-\tOccurrences: 3155\n",
    "*  8\tcome\t-\tOccurrences: 3125\n",
    "*  9\tgood\t-\tOccurrences: 2835\n",
    "* 10\tsir\t\t-\tOccurrences: 2803\n",
    "* 11\tlet\t\t-\tOccurrences: 2371\n",
    "* 12\tlove\t-\tOccurrences: 2371\n",
    "* 13\tenter\t-\tOccurrences: 2122\n",
    "* 14\tman\t\t-\tOccurrences: 2004\n",
    "* 15\tmake\t-\tOccurrences: 1992\n",
    "* 16\thath\t-\tOccurrences: 1939\n",
    "* 17\tsay\t\t-\tOccurrences: 1925\n",
    "* 18\tknow\t-\tOccurrences: 1880\n",
    "* 19\tlike\t-\tOccurrences: 1729\n",
    "* 20\tdid\t\t-\tOccurrences: 1626\n",
    "* 21\tspeak\t-\tOccurrences: 1297\n",
    "* 22\ttime\t-\tOccurrences: 1287\n",
    "* 23\theart\t-\tOccurrences: 1238\n",
    "* 24\tgod\t\t-\tOccurrences: 1184\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
